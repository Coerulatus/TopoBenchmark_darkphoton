{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topomodelx\n",
    "import toponetx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.datasets as geom_datasets\n",
    "\n",
    "from topomodelx.nn.hypergraph.allset import AllSet\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = geom_datasets.Planetoid(root=\"tmp/\", name=\"cora\")\n",
    "data = cora.data\n",
    "\n",
    "x_0s = data.x\n",
    "y = data.y\n",
    "edge_index = data.edge_index\n",
    "\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the graph is undirected (optional but often useful for one-hop neighborhoods).\n",
    "edge_index = to_undirected(edge_index)\n",
    "\n",
    "# Create a list of one-hop neighborhoods for each node.\n",
    "one_hop_neighborhoods = []\n",
    "for node in range(data.num_nodes):\n",
    "    # Get the one-hop neighbors of the current node.\n",
    "    neighbors = data.edge_index[1, data.edge_index[0] == node]\n",
    "\n",
    "    # Append the neighbors to the list of one-hop neighborhoods.\n",
    "    one_hop_neighborhoods.append(neighbors.numpy())\n",
    "\n",
    "# Detect and eliminate duplicate hyperedges.\n",
    "unique_hyperedges = set()\n",
    "hyperedges = []\n",
    "for neighborhood in one_hop_neighborhoods:\n",
    "    # Sort the neighborhood to ensure consistent comparison.\n",
    "    neighborhood = tuple(sorted(neighborhood))\n",
    "    if neighborhood not in unique_hyperedges:\n",
    "        hyperedges.append(list(neighborhood))\n",
    "        unique_hyperedges.add(neighborhood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_edges = len(hyperedges)\n",
    "incidence_1 = np.zeros((x_0s.shape[0], max_edges))\n",
    "for col, neighibourhood in enumerate(hyperedges):\n",
    "    for row in neighibourhood:\n",
    "        incidence_1[row, col] = 1\n",
    "\n",
    "assert all(incidence_1.sum(0) > 0) == True, \"Some hyperedges are empty\"\n",
    "assert all(incidence_1.sum(1) > 0) == True, \"Some nodes are not in any hyperedges\"\n",
    "incidence_1 = torch.Tensor(incidence_1).to_sparse_coo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    \"\"\"Network class that initializes the AllSet model and readout layer.\n",
    "\n",
    "    Base model parameters:\n",
    "    ----------\n",
    "    Reqired:\n",
    "    in_channels : int\n",
    "        Dimension of the input features.\n",
    "    hidden_channels : int\n",
    "        Dimension of the hidden features.\n",
    "\n",
    "    Optitional:\n",
    "    **kwargs : dict\n",
    "        Additional arguments for the base model.\n",
    "\n",
    "    Readout layer parameters:\n",
    "    ----------\n",
    "    out_channels : int\n",
    "        Dimension of the output features.\n",
    "    task_level : str\n",
    "        Level of the task. Either \"graph\" or \"node\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels, hidden_channels, out_channels, task_level=\"graph\", **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the model\n",
    "        self.base_model = AllSet(\n",
    "            in_channels=in_channels, hidden_channels=hidden_channels, **kwargs\n",
    "        )\n",
    "\n",
    "        # Readout\n",
    "        self.linear = torch.nn.Linear(hidden_channels, out_channels)\n",
    "        self.out_pool = True if task_level == \"graph\" else False\n",
    "\n",
    "    def forward(self, x_0, incidence_1):\n",
    "        # Base model\n",
    "        x_0, x_1 = self.base_model(x_0, incidence_1)\n",
    "\n",
    "        # Pool over all nodes in the hypergraph\n",
    "        if self.out_pool is True:\n",
    "            x = torch.max(x_0, dim=0)[0]\n",
    "        else:\n",
    "            x = x_0\n",
    "\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model hyperparameters\n",
    "in_channels = x_0s.shape[1]\n",
    "hidden_channels = 128\n",
    "n_layers = 1\n",
    "mlp_num_layers = 1\n",
    "\n",
    "# Readout hyperparameters\n",
    "out_channels = torch.unique(y).shape[0]\n",
    "task_level = \"graph\" if out_channels == 1 else \"node\"\n",
    "\n",
    "\n",
    "model = Network(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    n_layers=n_layers,\n",
    "    mlp_num_layers=mlp_num_layers,\n",
    "    task_level=task_level,\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Categorial cross-entropy loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Accuracy\n",
    "acc_fn = lambda y, y_hat: (y == y_hat).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2699/4129202519.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_0s = torch.tensor(x_0s)\n",
      "/tmp/ipykernel_2699/4129202519.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(y, dtype=torch.long).to(device),\n"
     ]
    }
   ],
   "source": [
    "x_0s = torch.tensor(x_0s)\n",
    "x_0s, incidence_1, y = (\n",
    "    x_0s.float().to(device),\n",
    "    incidence_1.float().to(device),\n",
    "    torch.tensor(y, dtype=torch.long).to(device),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \n",
      "Train_loss: 0.2473, acc: 0.8643\n",
      "Val_loss: 3.5598, Val_acc: 0.5320\n",
      "Test_loss: 3.2767, Test_acc: 0.5850\n",
      "Epoch: 4 \n",
      "Train_loss: 0.3580, acc: 0.8286\n",
      "Val_loss: 3.1860, Val_acc: 0.6480\n",
      "Test_loss: 3.2899, Test_acc: 0.6310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \n",
      "Train_loss: 0.5759, acc: 0.9929\n",
      "Val_loss: 3.1084, Val_acc: 0.6700\n",
      "Test_loss: 2.7987, Test_acc: 0.6670\n",
      "Epoch: 8 \n",
      "Train_loss: 0.4760, acc: 0.8857\n",
      "Val_loss: 2.5174, Val_acc: 0.6440\n",
      "Test_loss: 2.3115, Test_acc: 0.6360\n",
      "Epoch: 10 \n",
      "Train_loss: 0.4477, acc: 1.0000\n",
      "Val_loss: 2.0274, Val_acc: 0.7040\n",
      "Test_loss: 1.8696, Test_acc: 0.6960\n",
      "Epoch: 12 \n",
      "Train_loss: 0.3760, acc: 0.9857\n",
      "Val_loss: 2.1582, Val_acc: 0.6920\n",
      "Test_loss: 1.9894, Test_acc: 0.6850\n",
      "Epoch: 14 \n",
      "Train_loss: 0.3331, acc: 0.9929\n",
      "Val_loss: 2.1305, Val_acc: 0.6960\n",
      "Test_loss: 1.9123, Test_acc: 0.6960\n",
      "Epoch: 16 \n",
      "Train_loss: 0.3083, acc: 1.0000\n",
      "Val_loss: 1.9562, Val_acc: 0.7100\n",
      "Test_loss: 1.7645, Test_acc: 0.7180\n",
      "Epoch: 18 \n",
      "Train_loss: 0.2763, acc: 0.9929\n",
      "Val_loss: 1.9223, Val_acc: 0.7180\n",
      "Test_loss: 1.7601, Test_acc: 0.7200\n",
      "Epoch: 20 \n",
      "Train_loss: 0.2511, acc: 1.0000\n",
      "Val_loss: 1.9127, Val_acc: 0.7120\n",
      "Test_loss: 1.7811, Test_acc: 0.7160\n",
      "Epoch: 22 \n",
      "Train_loss: 0.2294, acc: 1.0000\n",
      "Val_loss: 1.9621, Val_acc: 0.7120\n",
      "Test_loss: 1.8588, Test_acc: 0.7080\n",
      "Epoch: 24 \n",
      "Train_loss: 0.2106, acc: 1.0000\n",
      "Val_loss: 2.0593, Val_acc: 0.7040\n",
      "Test_loss: 1.9770, Test_acc: 0.6930\n",
      "Epoch: 26 \n",
      "Train_loss: 0.1947, acc: 1.0000\n",
      "Val_loss: 2.1657, Val_acc: 0.6980\n",
      "Test_loss: 2.0981, Test_acc: 0.6870\n",
      "Epoch: 28 \n",
      "Train_loss: 0.1811, acc: 1.0000\n",
      "Val_loss: 2.2621, Val_acc: 0.6940\n",
      "Test_loss: 2.1960, Test_acc: 0.6860\n",
      "Epoch: 30 \n",
      "Train_loss: 0.1691, acc: 1.0000\n",
      "Val_loss: 2.3490, Val_acc: 0.6920\n",
      "Test_loss: 2.2767, Test_acc: 0.6860\n"
     ]
    }
   ],
   "source": [
    "test_interval = 2\n",
    "num_epochs = 30\n",
    "\n",
    "epoch_loss = []\n",
    "for epoch_i in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Extract edge_index from sparse incidence matrix\n",
    "    y_hat = model(x_0s, incidence_1)\n",
    "    loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    epoch_loss.append(loss.item())\n",
    "\n",
    "    if epoch_i % test_interval == 0:\n",
    "        model.eval()\n",
    "        y_hat = model(x_0s, incidence_1)\n",
    "\n",
    "        loss = loss_fn(y_hat[train_mask], y[train_mask])\n",
    "        print(f\"Epoch: {epoch_i} \")\n",
    "        print(\n",
    "            f\"Train_loss: {np.mean(epoch_loss):.4f}, acc: {acc_fn(y_hat[train_mask].argmax(1), y[train_mask]):.4f}\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(y_hat[val_mask], y[val_mask])\n",
    "\n",
    "        print(\n",
    "            f\"Val_loss: {loss:.4f}, Val_acc: {acc_fn(y_hat[val_mask].argmax(1), y[val_mask]):.4f}\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(y_hat[test_mask], y[test_mask])\n",
    "        print(\n",
    "            f\"Test_loss: {loss:.4f}, Test_acc: {acc_fn(y_hat[test_mask].argmax(1), y[test_mask]):.4f}\",\n",
    "            flush=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
