{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_418258/724119490.py:20: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../configs\", job_name=\"job\")\n"
     ]
    }
   ],
   "source": [
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(\"./\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from topobenchmarkx.data.datasets import CustomDataset\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from topobenchmarkx.data.dataloader_fullbatch import FullBatchDataModule\n",
    "from topobenchmarkx.data.load.loaders import (\n",
    "    GraphLoader,\n",
    "    SimplicialLoader,\n",
    "    HypergraphLoader,\n",
    ")\n",
    "\n",
    "initialize(config_path=\"../configs\", job_name=\"job\")\n",
    "config = compose(config_name=\"train.yaml\", return_hydra_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_target_': 'topobenchmarkx.transforms.transformchain.TransformChain', 'transform1': {'_target_': 'topobenchmarkx.transforms.lifting.DataLiftingTransform', 'type': 'Null Lifting (Identity)', 'lifting': 'Identity'}, 'transform2': {'_target_': 'topobenchmarkx.transforms.lifting.DataLiftingTransform', 'lifting': 'HypergraphKHopLifting', 'k_value': 1}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dataset.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: /TopoBenchmarkX/datasets/graph/cocitation/Cora/transform1_transform2_/2303687994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:290: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "a = hydra.utils.instantiate(config.dataset, _recursive_=False)\n",
    "dataset = a.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(indices=tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "                         [   0,  633, 1862,  ..., 1473, 2706, 2707]]),\n",
       "         values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n",
       "         size=(2708, 2708), nnz=13264, layout=torch.sparse_coo),\n",
       "  tensor([1890,  790, 2458, 2352,  366, 2543, 1815, 2692, 2541,  228,  912, 2179,\n",
       "           417, 1230,  742,   79, 1214,  492, 1909, 1169, 1342,  824, 2240, 2151,\n",
       "          1481, 2574, 2405, 1024,  592, 2301, 2524, 2445, 1797, 2101,  127, 2297,\n",
       "           403,  150, 2614, 1437, 1629,   62,   95,  959, 2122, 1576, 2132,  236,\n",
       "           222, 2542, 1810,  120, 1685,  193,  181,  389,  968, 2219, 1545,  669,\n",
       "          1365, 1394,  331,  614, 1286,  266, 1932, 1807,  225,  105, 1508,  630,\n",
       "          2068,  870, 2098, 1186, 1891, 1755, 1926,  707, 2288, 1688,  284, 1659,\n",
       "           920, 1433, 2328, 2191,  734, 1812, 2681,  922, 1402, 2548,  167, 2519,\n",
       "            12, 1546,  617, 1092,   90,  612, 2453, 1404,  857, 2233,  777, 1638,\n",
       "          1986, 2570,   28, 1522, 1066, 1911,   74,  515, 1309, 2002, 1584, 1517,\n",
       "           116, 1072, 1799, 2462, 1600,  622,  409, 1623,  115,  288, 2475, 1181,\n",
       "           337,   46,  747, 2481,  753,  462, 1938,  673, 1201, 2432,  424,  325,\n",
       "           664,  350,  447,  709, 1924, 1722, 1221, 1149,  387, 1369, 1337,  754,\n",
       "          2186,  451, 1194,  699, 1608,  830,  950, 2398, 1734, 1413, 1409, 2691,\n",
       "          2396, 1177, 2363, 2006, 1253,   21, 1305, 2484,  671, 2547, 1941,  845,\n",
       "           694, 2238, 1353, 1133,  869,  604,  834, 2220,  627,  902, 1747, 2051,\n",
       "           321,  696, 1179,  624, 2517, 2173, 1011, 1480, 1090,  848, 2276,  780,\n",
       "           605,  174, 1202,  100, 1243,  741,  334, 1705,  594, 1333, 2214, 2626,\n",
       "          1551, 2267, 1542, 2304,  327, 1933,  640, 2152, 2042, 1468, 1838,  460,\n",
       "          1046, 1398, 1562, 1975,  419,  738, 1879, 1389, 1438,  497,  375,  975,\n",
       "          1209, 2387,  404, 1425,   93, 1769, 2221, 1849,  827,  428, 1889, 2520,\n",
       "           282,  343, 1113, 1574,  691,    0, 1582, 1707, 1416, 2021, 2003, 1993,\n",
       "          1131, 1247, 2407, 2150, 2261, 2703, 1159,  246, 2689, 2612, 1671,  929,\n",
       "          2483, 1461, 1971,  365, 2296,  736,   26, 1441,  680, 1250,    3, 2182,\n",
       "           837,  292,  698, 1048, 2505, 1667, 1963, 1249, 2274, 1298, 1868, 1443,\n",
       "           603, 1289,   63, 1579, 1883,  990, 1384,  237,  650, 1967,  112, 1348,\n",
       "           804, 1556,  577, 1834,  126, 2414, 2472,  851, 2185,    7,  854, 2584,\n",
       "          2464,  407,  153, 1566, 2353, 1004, 2401,  280,  400, 1596, 1497, 1479,\n",
       "          1800, 2009,  504, 1744,  903, 2040, 1672,  550, 1862, 1805, 2439, 1130,\n",
       "           945,  163, 1660, 1690, 2107, 2619, 1514,  449,  941, 2563, 1532, 1847,\n",
       "           437, 1314,  786,  873,  739, 1006, 1844,  586, 1306,  348, 1028, 1163,\n",
       "          2623, 1541,  573,  647,  595, 1251, 1558, 1308,  470,  800,  281,  932,\n",
       "           732, 2470, 1144, 1617, 2423,  584, 2195,  160, 1111, 1915,  859,  952,\n",
       "          2478, 1331, 1219, 1215,  606,  164, 2154, 1040, 1978, 2597,  329, 2011,\n",
       "          2613, 1429, 2508, 2141,  809,  433, 1107, 2091,  860,  274,  967, 2676,\n",
       "          2139, 1346,  885,  556,  908, 1381, 2117,  509, 2388,  221, 1297,  146,\n",
       "          1777, 1053, 1822,  508,  323, 1561,  176, 1908,  168, 1400, 2650, 1392,\n",
       "          2257, 2177, 2237,  136, 1470, 1278, 1973,  894,  197, 2338, 1483, 1792,\n",
       "          2084,  767, 2280, 2081, 1112, 2620,  865, 2389, 1085, 1447,  199,  429,\n",
       "          1823, 1795, 1970,  730,  297, 2121,  807, 1565, 1172, 2586, 2592,  821,\n",
       "          1241, 1563, 2577,  623, 1445, 1718, 1162, 2260, 1640, 2590, 1022, 1750,\n",
       "           291, 2022, 1740, 2282, 1007,   67,   24,  216, 1910, 1153, 2159,  166,\n",
       "           207, 2322, 2089,  591, 1176, 1720, 1863,  525, 2335,  469, 1350, 2169,\n",
       "          1531,  928, 1896, 1619, 1469, 1141, 1913,  956, 1488,   25, 1134, 2197,\n",
       "           925, 1552,  843, 1645, 1020, 2187,  749, 2371,  180, 1272, 2498, 1434,\n",
       "           607, 1352, 2344, 1904, 1899,   94,  770,  637, 1304, 2334, 2536, 2492,\n",
       "          1208, 1395, 1578, 1198,  275, 1152, 1206, 2080, 1104,  307, 2059,  872,\n",
       "            86, 1435,  209, 1966, 1987, 2171,  931, 2146, 2425,  130, 1723, 2071,\n",
       "           954,  555, 2418, 1930, 1536, 2431,  201, 2023,  368,  973, 1634, 2435,\n",
       "           816, 1954, 1316, 1905,  256, 1472, 1699, 2217, 1605, 1329,  560, 1877,\n",
       "           373, 1684, 1167, 2024, 1204,  972, 1155, 2036, 2675, 1071,  774,  324,\n",
       "          2251,   84, 1589, 2446, 1920,   91,  430, 2339, 2105, 1345,  257,  888,\n",
       "          2673, 1466, 1641,  388, 2383,  273,  756, 1925, 1956, 2665, 2176,  802,\n",
       "          2076, 1207, 2558,  151, 2292,  714, 2591,  544,  639,  423,  659,  797,\n",
       "          2685, 2008,   99, 2496, 1871, 2046,  755,  976, 2163, 1940, 2120,  537,\n",
       "          1701, 2222, 2135, 2599,  705, 2362, 2648, 2647, 1828, 1778,  277, 1033,\n",
       "          1731,  763,  835, 1653, 2607]),\n",
       "  tensor([1363,  178,  498,  340, 1359, 1884, 2442,  198, 2550, 2085, 1503, 1166,\n",
       "          1848, 2671, 2663, 1339, 2201,  759,  170, 1356,  773, 2326, 2259,  992,\n",
       "            45, 1689,  467, 1759, 2155, 1977, 1902,  319,  184, 1739, 1538, 1673,\n",
       "          1450, 1509,  839, 1540, 2269,  744,  539, 1154,  801, 2307, 1135,  210,\n",
       "          1621, 1639,   65,  863, 1632, 1651,  203, 1132, 1021, 2033, 1026, 2696,\n",
       "           341, 2086,  875, 1120,  372,  576,  942,  211,  520,  939,  889, 1258,\n",
       "          2341, 1591, 1998, 1325,  354,   96, 2477,  589, 2683, 2497, 2367,  513,\n",
       "            71, 1577, 2518, 2109, 1895,  456,  725,   68, 1525, 1917, 1458,  881,\n",
       "          1581,  231,  377, 2196,  862,  432, 1460, 2506,  994, 1858, 1559, 1378,\n",
       "          1143, 1758,  172, 2250, 1248,  814,  208,  218,  948,   16, 2054,  367,\n",
       "          1406, 1128,  884, 1319, 1388, 2422, 2529, 1567, 2366, 2210, 1573,  394,\n",
       "          1921,  984,  542,  803,  677,  406, 1459,  552,  362,  549,  970, 2143,\n",
       "          1900,  940, 2638, 2299,  788, 2013,  355, 1299, 2654, 1570, 1652, 1709,\n",
       "           642, 1590,  554, 2554,  693, 1995, 2393,  649, 2244, 1922,  420,  593,\n",
       "           559,  339, 1603, 2582, 2415,  706, 2321, 2029, 2160, 1444, 2653,  960,\n",
       "          1265, 1164, 2465, 2399, 1903, 1493, 1484,  500,  147,  258,  769,   13,\n",
       "          1804, 2063, 1959,  114,  957, 2559,  241, 2598, 1354,  679, 2701,   56,\n",
       "          1252, 2596,  435, 1765, 2637,  155, 2705, 1361, 1016,   60,   29,  782,\n",
       "            35, 2489, 1446, 1171, 1310, 1060,  358, 1123,  499,  524, 1200,  737,\n",
       "            50,  265, 1820, 1315, 1518, 1681, 2119, 2303, 1729,  336,  548, 2129,\n",
       "          1893,  887, 2636,  494,  580, 1809, 2057,  431, 1189, 1229, 2216, 2245,\n",
       "           212, 2058, 1064, 1245,   75, 2165, 1312, 2455, 1031, 2360,  247, 1595,\n",
       "          1869, 1500,  186, 1771,  668, 1642, 1336,    8, 1622, 1780, 1075, 2180,\n",
       "           631,  293, 2097, 1105,   32,  134, 1124,  445,  226,  464,  455, 2429,\n",
       "          2678, 1794,  999, 1786, 2096,  752, 2633, 2324,  533,  243,  401, 1495,\n",
       "           585,  244, 1837, 2406, 1340, 1501,  450, 1227, 1182, 2114, 1818,  285,\n",
       "           471, 2031, 1637, 1850, 2461, 2342, 1678,  304,  871, 1835,  611, 1714,\n",
       "          1407,  426, 1184, 1790, 1014, 1691, 2106, 2145, 1781, 2500, 2078, 1670,\n",
       "          2421,  678, 1044, 2660,  943, 1218, 1498, 1065, 1593, 2284, 1892,   49,\n",
       "           540, 1668,  110, 1631, 2390,  510, 1887, 2158,   44, 1115, 1269,  235,\n",
       "          1324, 1607,   78,  269, 1721,  826,  313, 1942, 1307, 1351, 1564,  911,\n",
       "          1430,  697,  338,  466, 1019,  363, 2148, 2532, 1344, 1360, 2300, 2204,\n",
       "          1094, 2232, 2657, 1524, 1034,  685, 1420, 1529,  819, 1843,   83, 2684,\n",
       "          1311, 1088, 1990, 1439, 2642,  919, 1440, 2564, 2066, 1785,  205,  328,\n",
       "           808, 1510, 1196, 1057,  545,  528, 1281,  656, 2521, 1442, 2565,  858,\n",
       "          1839, 2205, 1886,  844,  833,  879, 1614, 1749, 1798,  213,  472, 2378,\n",
       "           131, 1039,  439,  849, 2631,  476, 1419,  904, 1609, 1752,  853,  448,\n",
       "          2410, 1717,  129, 2190, 1727,  103,  635, 1084,  784, 1927,  969, 1803,\n",
       "          1102, 1415,  165, 2317, 1260, 1618, 1742,  907, 2094,  349, 2271, 1494,\n",
       "           371, 2552, 1992, 1276,  507, 1624, 1335, 1741, 2188,  395,  625,  695,\n",
       "           947,  761, 1382, 2309,  263, 1383, 2567, 2330,  846,  783, 1401,  645,\n",
       "          2513, 2070, 1658, 2649,  771, 1754, 2434,   51, 2374,  493,  601, 2641,\n",
       "           301, 1375, 1121, 2468,  993, 1035, 2067,  238,   97,  779, 2316,  111,\n",
       "          1220, 1636,  183, 2544, 1268,  613, 2050,  441, 1706, 1557, 2617, 1711,\n",
       "          1178,   81,  484,  397, 1291, 1613, 1875, 1410, 1964, 2581, 2379, 1156,\n",
       "          2332,  370, 1364, 1743,   88, 2625,   38, 2531, 1994, 1700,  571, 2706,\n",
       "          2386, 2077,  934, 1318,  261, 1290, 2293,  514, 1534, 2440,  599, 1874,\n",
       "           356,  608, 2211,  239, 1223, 2629, 2283,  119, 2306, 2069, 2266, 1814,\n",
       "           751,  915,  374, 1477, 2557, 1067, 2056, 1937, 1321, 1125,  463, 1527,\n",
       "          1991, 1158, 2184,  729,  410, 2476, 1391, 1829, 2419, 1408, 1224, 2037,\n",
       "          1089, 2466, 1693, 1191,  867, 1296, 2530,  382, 2198, 1262, 1630, 2099,\n",
       "           541, 1502,  490,   59, 1320, 2287, 1233,  188, 2667,  681, 2606, 1274,\n",
       "          2053,  316, 1238, 1475,  123, 2194,  874,  660, 1796,  531,  255, 1054,\n",
       "           662, 1960, 2369,  230,  901, 2162,  765, 2669, 2212,  380, 2181, 1745,\n",
       "           864, 1288, 2575,  787,  101, 1037, 2391, 2130,  421, 2226, 1851,  877,\n",
       "          1095,  675, 1873,  566,  290, 2131,  798, 1870, 2007,  523,  626,  495,\n",
       "          1583, 2225, 1616, 1782,  158]),\n",
       "  tensor([1808, 1918,  891,  ...,  632, 2646,  955]),\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  tensor([3, 4, 4,  ..., 3, 3, 3]),\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  2708],\n",
       " ['incidence_1',\n",
       "  'test_mask',\n",
       "  'val_mask',\n",
       "  'train_mask',\n",
       "  'x',\n",
       "  'x_hyperedges',\n",
       "  'y',\n",
       "  'x_0',\n",
       "  'num_hyperedges'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "/usr/local/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:290: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([1363,  178,  498,  340, 1359, 1884, 2442,  198, 2550, 2085, 1503, 1166,\n",
       "          1848, 2671, 2663, 1339, 2201,  759,  170, 1356,  773, 2326, 2259,  992,\n",
       "            45, 1689,  467, 1759, 2155, 1977, 1902,  319,  184, 1739, 1538, 1673,\n",
       "          1450, 1509,  839, 1540, 2269,  744,  539, 1154,  801, 2307, 1135,  210,\n",
       "          1621, 1639,   65,  863, 1632, 1651,  203, 1132, 1021, 2033, 1026, 2696,\n",
       "           341, 2086,  875, 1120,  372,  576,  942,  211,  520,  939,  889, 1258,\n",
       "          2341, 1591, 1998, 1325,  354,   96, 2477,  589, 2683, 2497, 2367,  513,\n",
       "            71, 1577, 2518, 2109, 1895,  456,  725,   68, 1525, 1917, 1458,  881,\n",
       "          1581,  231,  377, 2196,  862,  432, 1460, 2506,  994, 1858, 1559, 1378,\n",
       "          1143, 1758,  172, 2250, 1248,  814,  208,  218,  948,   16, 2054,  367,\n",
       "          1406, 1128,  884, 1319, 1388, 2422, 2529, 1567, 2366, 2210, 1573,  394,\n",
       "          1921,  984,  542,  803,  677,  406, 1459,  552,  362,  549,  970, 2143,\n",
       "          1900,  940, 2638, 2299,  788, 2013,  355, 1299, 2654, 1570, 1652, 1709,\n",
       "           642, 1590,  554, 2554,  693, 1995, 2393,  649, 2244, 1922,  420,  593,\n",
       "           559,  339, 1603, 2582, 2415,  706, 2321, 2029, 2160, 1444, 2653,  960,\n",
       "          1265, 1164, 2465, 2399, 1903, 1493, 1484,  500,  147,  258,  769,   13,\n",
       "          1804, 2063, 1959,  114,  957, 2559,  241, 2598, 1354,  679, 2701,   56,\n",
       "          1252, 2596,  435, 1765, 2637,  155, 2705, 1361, 1016,   60,   29,  782,\n",
       "            35, 2489, 1446, 1171, 1310, 1060,  358, 1123,  499,  524, 1200,  737,\n",
       "            50,  265, 1820, 1315, 1518, 1681, 2119, 2303, 1729,  336,  548, 2129,\n",
       "          1893,  887, 2636,  494,  580, 1809, 2057,  431, 1189, 1229, 2216, 2245,\n",
       "           212, 2058, 1064, 1245,   75, 2165, 1312, 2455, 1031, 2360,  247, 1595,\n",
       "          1869, 1500,  186, 1771,  668, 1642, 1336,    8, 1622, 1780, 1075, 2180,\n",
       "           631,  293, 2097, 1105,   32,  134, 1124,  445,  226,  464,  455, 2429,\n",
       "          2678, 1794,  999, 1786, 2096,  752, 2633, 2324,  533,  243,  401, 1495,\n",
       "           585,  244, 1837, 2406, 1340, 1501,  450, 1227, 1182, 2114, 1818,  285,\n",
       "           471, 2031, 1637, 1850, 2461, 2342, 1678,  304,  871, 1835,  611, 1714,\n",
       "          1407,  426, 1184, 1790, 1014, 1691, 2106, 2145, 1781, 2500, 2078, 1670,\n",
       "          2421,  678, 1044, 2660,  943, 1218, 1498, 1065, 1593, 2284, 1892,   49,\n",
       "           540, 1668,  110, 1631, 2390,  510, 1887, 2158,   44, 1115, 1269,  235,\n",
       "          1324, 1607,   78,  269, 1721,  826,  313, 1942, 1307, 1351, 1564,  911,\n",
       "          1430,  697,  338,  466, 1019,  363, 2148, 2532, 1344, 1360, 2300, 2204,\n",
       "          1094, 2232, 2657, 1524, 1034,  685, 1420, 1529,  819, 1843,   83, 2684,\n",
       "          1311, 1088, 1990, 1439, 2642,  919, 1440, 2564, 2066, 1785,  205,  328,\n",
       "           808, 1510, 1196, 1057,  545,  528, 1281,  656, 2521, 1442, 2565,  858,\n",
       "          1839, 2205, 1886,  844,  833,  879, 1614, 1749, 1798,  213,  472, 2378,\n",
       "           131, 1039,  439,  849, 2631,  476, 1419,  904, 1609, 1752,  853,  448,\n",
       "          2410, 1717,  129, 2190, 1727,  103,  635, 1084,  784, 1927,  969, 1803,\n",
       "          1102, 1415,  165, 2317, 1260, 1618, 1742,  907, 2094,  349, 2271, 1494,\n",
       "           371, 2552, 1992, 1276,  507, 1624, 1335, 1741, 2188,  395,  625,  695,\n",
       "           947,  761, 1382, 2309,  263, 1383, 2567, 2330,  846,  783, 1401,  645,\n",
       "          2513, 2070, 1658, 2649,  771, 1754, 2434,   51, 2374,  493,  601, 2641,\n",
       "           301, 1375, 1121, 2468,  993, 1035, 2067,  238,   97,  779, 2316,  111,\n",
       "          1220, 1636,  183, 2544, 1268,  613, 2050,  441, 1706, 1557, 2617, 1711,\n",
       "          1178,   81,  484,  397, 1291, 1613, 1875, 1410, 1964, 2581, 2379, 1156,\n",
       "          2332,  370, 1364, 1743,   88, 2625,   38, 2531, 1994, 1700,  571, 2706,\n",
       "          2386, 2077,  934, 1318,  261, 1290, 2293,  514, 1534, 2440,  599, 1874,\n",
       "           356,  608, 2211,  239, 1223, 2629, 2283,  119, 2306, 2069, 2266, 1814,\n",
       "           751,  915,  374, 1477, 2557, 1067, 2056, 1937, 1321, 1125,  463, 1527,\n",
       "          1991, 1158, 2184,  729,  410, 2476, 1391, 1829, 2419, 1408, 1224, 2037,\n",
       "          1089, 2466, 1693, 1191,  867, 1296, 2530,  382, 2198, 1262, 1630, 2099,\n",
       "           541, 1502,  490,   59, 1320, 2287, 1233,  188, 2667,  681, 2606, 1274,\n",
       "          2053,  316, 1238, 1475,  123, 2194,  874,  660, 1796,  531,  255, 1054,\n",
       "           662, 1960, 2369,  230,  901, 2162,  765, 2669, 2212,  380, 2181, 1745,\n",
       "           864, 1288, 2575,  787,  101, 1037, 2391, 2130,  421, 2226, 1851,  877,\n",
       "          1095,  675, 1873,  566,  290, 2131,  798, 1870, 2007,  523,  626,  495,\n",
       "          1583, 2225, 1616, 1782,  158]),\n",
       "  tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  tensor([1808, 1918,  891,  ...,  632, 2646,  955]),\n",
       "  tensor([3, 4, 4,  ..., 3, 3, 3]),\n",
       "  tensor([1890,  790, 2458, 2352,  366, 2543, 1815, 2692, 2541,  228,  912, 2179,\n",
       "           417, 1230,  742,   79, 1214,  492, 1909, 1169, 1342,  824, 2240, 2151,\n",
       "          1481, 2574, 2405, 1024,  592, 2301, 2524, 2445, 1797, 2101,  127, 2297,\n",
       "           403,  150, 2614, 1437, 1629,   62,   95,  959, 2122, 1576, 2132,  236,\n",
       "           222, 2542, 1810,  120, 1685,  193,  181,  389,  968, 2219, 1545,  669,\n",
       "          1365, 1394,  331,  614, 1286,  266, 1932, 1807,  225,  105, 1508,  630,\n",
       "          2068,  870, 2098, 1186, 1891, 1755, 1926,  707, 2288, 1688,  284, 1659,\n",
       "           920, 1433, 2328, 2191,  734, 1812, 2681,  922, 1402, 2548,  167, 2519,\n",
       "            12, 1546,  617, 1092,   90,  612, 2453, 1404,  857, 2233,  777, 1638,\n",
       "          1986, 2570,   28, 1522, 1066, 1911,   74,  515, 1309, 2002, 1584, 1517,\n",
       "           116, 1072, 1799, 2462, 1600,  622,  409, 1623,  115,  288, 2475, 1181,\n",
       "           337,   46,  747, 2481,  753,  462, 1938,  673, 1201, 2432,  424,  325,\n",
       "           664,  350,  447,  709, 1924, 1722, 1221, 1149,  387, 1369, 1337,  754,\n",
       "          2186,  451, 1194,  699, 1608,  830,  950, 2398, 1734, 1413, 1409, 2691,\n",
       "          2396, 1177, 2363, 2006, 1253,   21, 1305, 2484,  671, 2547, 1941,  845,\n",
       "           694, 2238, 1353, 1133,  869,  604,  834, 2220,  627,  902, 1747, 2051,\n",
       "           321,  696, 1179,  624, 2517, 2173, 1011, 1480, 1090,  848, 2276,  780,\n",
       "           605,  174, 1202,  100, 1243,  741,  334, 1705,  594, 1333, 2214, 2626,\n",
       "          1551, 2267, 1542, 2304,  327, 1933,  640, 2152, 2042, 1468, 1838,  460,\n",
       "          1046, 1398, 1562, 1975,  419,  738, 1879, 1389, 1438,  497,  375,  975,\n",
       "          1209, 2387,  404, 1425,   93, 1769, 2221, 1849,  827,  428, 1889, 2520,\n",
       "           282,  343, 1113, 1574,  691,    0, 1582, 1707, 1416, 2021, 2003, 1993,\n",
       "          1131, 1247, 2407, 2150, 2261, 2703, 1159,  246, 2689, 2612, 1671,  929,\n",
       "          2483, 1461, 1971,  365, 2296,  736,   26, 1441,  680, 1250,    3, 2182,\n",
       "           837,  292,  698, 1048, 2505, 1667, 1963, 1249, 2274, 1298, 1868, 1443,\n",
       "           603, 1289,   63, 1579, 1883,  990, 1384,  237,  650, 1967,  112, 1348,\n",
       "           804, 1556,  577, 1834,  126, 2414, 2472,  851, 2185,    7,  854, 2584,\n",
       "          2464,  407,  153, 1566, 2353, 1004, 2401,  280,  400, 1596, 1497, 1479,\n",
       "          1800, 2009,  504, 1744,  903, 2040, 1672,  550, 1862, 1805, 2439, 1130,\n",
       "           945,  163, 1660, 1690, 2107, 2619, 1514,  449,  941, 2563, 1532, 1847,\n",
       "           437, 1314,  786,  873,  739, 1006, 1844,  586, 1306,  348, 1028, 1163,\n",
       "          2623, 1541,  573,  647,  595, 1251, 1558, 1308,  470,  800,  281,  932,\n",
       "           732, 2470, 1144, 1617, 2423,  584, 2195,  160, 1111, 1915,  859,  952,\n",
       "          2478, 1331, 1219, 1215,  606,  164, 2154, 1040, 1978, 2597,  329, 2011,\n",
       "          2613, 1429, 2508, 2141,  809,  433, 1107, 2091,  860,  274,  967, 2676,\n",
       "          2139, 1346,  885,  556,  908, 1381, 2117,  509, 2388,  221, 1297,  146,\n",
       "          1777, 1053, 1822,  508,  323, 1561,  176, 1908,  168, 1400, 2650, 1392,\n",
       "          2257, 2177, 2237,  136, 1470, 1278, 1973,  894,  197, 2338, 1483, 1792,\n",
       "          2084,  767, 2280, 2081, 1112, 2620,  865, 2389, 1085, 1447,  199,  429,\n",
       "          1823, 1795, 1970,  730,  297, 2121,  807, 1565, 1172, 2586, 2592,  821,\n",
       "          1241, 1563, 2577,  623, 1445, 1718, 1162, 2260, 1640, 2590, 1022, 1750,\n",
       "           291, 2022, 1740, 2282, 1007,   67,   24,  216, 1910, 1153, 2159,  166,\n",
       "           207, 2322, 2089,  591, 1176, 1720, 1863,  525, 2335,  469, 1350, 2169,\n",
       "          1531,  928, 1896, 1619, 1469, 1141, 1913,  956, 1488,   25, 1134, 2197,\n",
       "           925, 1552,  843, 1645, 1020, 2187,  749, 2371,  180, 1272, 2498, 1434,\n",
       "           607, 1352, 2344, 1904, 1899,   94,  770,  637, 1304, 2334, 2536, 2492,\n",
       "          1208, 1395, 1578, 1198,  275, 1152, 1206, 2080, 1104,  307, 2059,  872,\n",
       "            86, 1435,  209, 1966, 1987, 2171,  931, 2146, 2425,  130, 1723, 2071,\n",
       "           954,  555, 2418, 1930, 1536, 2431,  201, 2023,  368,  973, 1634, 2435,\n",
       "           816, 1954, 1316, 1905,  256, 1472, 1699, 2217, 1605, 1329,  560, 1877,\n",
       "           373, 1684, 1167, 2024, 1204,  972, 1155, 2036, 2675, 1071,  774,  324,\n",
       "          2251,   84, 1589, 2446, 1920,   91,  430, 2339, 2105, 1345,  257,  888,\n",
       "          2673, 1466, 1641,  388, 2383,  273,  756, 1925, 1956, 2665, 2176,  802,\n",
       "          2076, 1207, 2558,  151, 2292,  714, 2591,  544,  639,  423,  659,  797,\n",
       "          2685, 2008,   99, 2496, 1871, 2046,  755,  976, 2163, 1940, 2120,  537,\n",
       "          1701, 2222, 2135, 2599,  705, 2362, 2648, 2647, 1828, 1778,  277, 1033,\n",
       "          1731,  763,  835, 1653, 2607]),\n",
       "  tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "          [ 633, 1862, 2582,  ...,  598, 1473, 2706]])],\n",
       " ['val_mask', 'x', 'train_mask', 'y', 'test_mask', 'edge_index'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"transforms\" in config.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform parameters are the same, using existing data_dir: /Users/gbg141/Documents/TopoProjectX/TopoBenchmarkX/datasets/graph/cocitation/Cora/HypergraphKHopLifting/885927755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoBenchmarkX/venv_topox/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:293: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "data_loader = GraphLoader(config.dataset)\n",
    "data = data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], y=[2708], incidence_1=[2708, 2708], num_hyperedges=2708, x_0=[2708, 1433], x_hyperedges=[2708, 1433])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplicial_loader = SimplicialLoader(config.dataset)\n",
    "data = simplicial_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toponetx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shrec 16 small dataset...\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "shrec, _ = toponetx.datasets.mesh.shrec_16(size=\"small\")\n",
    "shrec = {key: np.array(value) for key, value in shrec.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicial Complex with shape (252, 750, 500) and dimension 2\n"
     ]
    }
   ],
   "source": [
    "print(shrec[\"complexes\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data\n",
    "# from topobenchmarkx.data.load.loaders import HypergraphLoader\n",
    "\n",
    "# data_loader = HypergraphLoader(config)\n",
    "# data = data_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hydra', 'task_name', 'tags', 'train', 'test', 'ckpt_path', 'seed', 'dataset', 'transforms', 'model', 'evaluator', 'callbacks', 'trainer', 'paths', 'extras'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_target_': 'topobenchmarkx.transforms.lifting.DataLiftingTransform', 'lifting': 'HypergraphKHopLifting', 'k_value': 1, 'complex_dim': 'None', 'max_triangles': 'None', 'aggregation_method': 'None'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"transforms\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifting = hydra.utils.instantiate(config.transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'k_value': 1,\n",
       " 'complex_dim': 'None',\n",
       " 'max_triangles': 'None',\n",
       " 'aggregation_method': 'None',\n",
       " 'lifting': 'HypergraphKHopLifting'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifting.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch_geometric.data.Data()\n",
    "data.x = torch.zeros([6, 1])\n",
    "data.edge_index = torch.tensor([[0, 0, 0, 1, 1, 1, 2, 4], [1, 2, 3, 2, 3, 4, 3, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lifted_data = lifting(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[6, 1], incidence_1=[6, 6], num_hyperedges=6, x_0=[6, 1], x_hyperedges=[6, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lifted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoBenchmarkX/venv_topox/lib/python3.11/site-packages/torch_geometric/data/dataset.py:234: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, pass `force_reload=True` explicitly to reload the dataset.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cora = torch_geometric.datasets.Planetoid(\n",
    "    root=\"../datasets/graph/\", name=\"Cora\", pre_transform=lifting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoBenchmarkX/venv_topox/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:293: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], num_cells_0=2708, num_cells_1=10556, num_cells_2=2648, incidence_1=[2708, 10556], incidence_2=[10556, 2648], laplacian_up_1=[10556, 10556], laplacian_up_2=[2648, 2648], laplacian_down_2=[10556, 10556], laplacian_down_1=[2708, 2708], x_0=[2708, 1433], x_1=[10556, 1433], x_2=[2648, 1433])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cora.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topobenchmarkx.data.datasets import CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_hyperedges=2708, x_0=[2708, 1433], incidence_1=[2708, 2708], x=[2708, 1433], x_hyperedges=[2708, 1433])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        batch - list of (tensor, label)\n",
    "\n",
    "    reutrn:\n",
    "        xs - a tensor of all examples in 'batch' after padding\n",
    "        ys - a LongTensor of all labels in batch\n",
    "    \"\"\"\n",
    "\n",
    "    for b in batch:\n",
    "        values, keys = b[0], b[1]\n",
    "        data = Data()\n",
    "        for key, value in zip(keys, values):\n",
    "            data[key] = value\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "d = DataLoader(dataset=CustomDataset([cora.data]), batch_size=1, collate_fn=collate_fn)\n",
    "next(iter(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/TopoBenchmarkX/notebooks/data.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6e696674795f73686f636b6c6579227d@ssh-remote%2Blevtel2/TopoBenchmarkX/notebooks/data.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m databatch \u001b[39m=\u001b[39m torch_geometric\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mBatch\u001b[39m.\u001b[39mfrom_data_list(data_lst)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_lst' is not defined"
     ]
    }
   ],
   "source": [
    "databatch = torch_geometric.data.Batch.from_data_list(data_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    1,    2,  ..., 2706, 2706, 2707],\n",
       "        [ 978,  736,  399,  ...,  362,  419,  921]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch.edge_index[:, :4905]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2708, 2709, 2710,  ..., 5414, 5414, 5415],\n",
       "        [3686, 3444, 3107,  ..., 3070, 3127, 3629]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch.edge_index[:, 4905:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AllSetTransformer(\n",
       "  (layers): ModuleList(\n",
       "    (0): AllSetTransformerLayer(\n",
       "      (vertex2edge): AllSetTransformerBlock(\n",
       "        (multihead_att): MultiHeadAttention()\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (1): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (edge2vertex): AllSetTransformerBlock(\n",
       "        (multihead_att): MultiHeadAttention()\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "          (1): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch.edge_index = databatch.edge_index.to_sparse()\n",
    "databatch.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_list, labels):\n",
    "        self.text_list = text_list\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text_list[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # You can perform any text preprocessing here if needed\n",
    "        # For example, tokenization, numerical encoding, etc.\n",
    "\n",
    "        return {\"text\": text, \"label\": label}\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text_data = [\n",
    "    \"This is an example.\",\n",
    "    \"Another text sample.\",\n",
    "    \"PyTorch DataLoader with text.\",\n",
    "]\n",
    "labels = [0, 1, 0]  # Example labels\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "custom_dataset = TextDataset(text_data, labels)\n",
    "\n",
    "# Use DataLoader to load batches of data\n",
    "batch_size = 2\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Iterate through batches\n",
    "for batch in data_loader:\n",
    "    texts = batch[\"text\"]\n",
    "    labels = batch[\"label\"]\n",
    "\n",
    "    # Perform your training/validation/test operations here\n",
    "    print(\"Texts:\", texts)\n",
    "    print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = hydra.utils.instantiate(config.model.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.utils.instantiate(config.model.backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReadOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readout = hydra.utils.instantiate(config.model.readout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x19ca5db60>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readout.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = hydra.utils.instantiate(config.evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gbg141/Documents/TopoProjectX/TopoBenchmarkX/venv_topox/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([0, 1, 2, 1, 2, 2]),\n",
       " 'logits': tensor([[ 1, 11, 11],\n",
       "         [ 5, 11,  3],\n",
       "         [ 2,  3,  4],\n",
       "         [ 5, 16,  7],\n",
       "         [ 8,  9, 10],\n",
       "         [11, 12, 13]]),\n",
       " 'metrics': {'acc': 0.8333333333333334,\n",
       "  'pre_micro': 0.8333333333333334,\n",
       "  'pre_macro': 0.5555555555555555,\n",
       "  'rec_micro': 0.8333333333333334,\n",
       "  'rec_macro': 0.6666666666666666,\n",
       "  'f1_micro': 0.8333333333333334,\n",
       "  'f1_macro': 0.6}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "d = {\n",
    "    \"labels\": torch.tensor([0, 1, 2, 1, 2, 2]),\n",
    "    \"logits\": torch.tensor(\n",
    "        [[1, 11, 11], [5, 11, 3], [2, 3, 4], [5, 16, 7], [8, 9, 10], [11, 12, 13]]\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "evaluator.eval(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[\"logits\"].argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hypergraph dataset name: cora\n",
      "number of nodes:2708, feature dimension: 1433\n",
      "number of hyperedges: 1072\n",
      "Final num_hyperedges 1392\n",
      "Final num_nodes 2708\n",
      "Final num_class 7\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/gbg141/Documents/TopoProjectX/TopoBenchmarkX/data//data_splits/cora/train_prop=0.5/split_0.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtopobenchmarkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader_fullbatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullBatchDataModule\n\u001b[1;32m      5\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m HypergraphLoader(config)\n\u001b[0;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m FullBatchDataModule(data\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/Documents/TopoProjectX/TopoBenchmarkX/topobenchmarkx/data/load/loaders.py:17\u001b[0m, in \u001b[0;36mHypergraphLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     15\u001b[0m ):\n\u001b[1;32m     16\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_hypergraph_pickle_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m---> 17\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mload_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# We need to add checks that:\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# All nodes belong to some edge, in case some not, create selfedge\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/Documents/TopoProjectX/TopoBenchmarkX/topobenchmarkx/data/utils.py:131\u001b[0m, in \u001b[0;36mload_split\u001b[0;34m(data, cfg)\u001b[0m\n\u001b[1;32m    129\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_split_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    130\u001b[0m load_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/split_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_seed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 131\u001b[0m splits \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m data\u001b[38;5;241m.\u001b[39mtrain_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(splits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    133\u001b[0m data\u001b[38;5;241m.\u001b[39mval_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(splits[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/TopoProjectX/TopoBenchmarkX/venv_topox/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/gbg141/Documents/TopoProjectX/TopoBenchmarkX/data//data_splits/cora/train_prop=0.5/split_0.npz'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from topobenchmarkx.data.load.loaders import HypergraphLoader\n",
    "from topobenchmarkx.data.dataloader_fullbatch import FullBatchDataModule\n",
    "\n",
    "data_loader = HypergraphLoader(config)\n",
    "data = data_loader.load()\n",
    "dataloader = FullBatchDataModule(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader.train_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.x[batch.train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hydra.utils.instantiate(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.hparams.backbone is a.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(a.hparams.backbone) == id(a.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.backbone.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b in []topomodelx.nn.hypergraph.unigcnii.UniGCNII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import topomodelx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
