# SAN
# Batch size =1
python train.py dataset=cocitation_cora model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_citeseer model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_pubmed model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
# Vary batch size
python train.py dataset=PROTEINS_TU model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=NCI1 model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=IMDB-BINARY model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=IMDB-MULTI model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=MUTAG model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=REDDIT-BINARY model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
# Fixed split
python train.py dataset=ZINC model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hid_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun



python train.py dataset=cocitation_cora model=cell/cwn model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2 model.backbone.n_filters=1,2,4 model.backbone.order_harmonic=1,2,4 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun