# EDHNN
python train.py dataset=cocitation_cora model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_citeseer model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_pubmed model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=ZINC model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=PROTEINS_TU model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0 logger.wandb.project=topobenchmark_0503 --multirun

# python train.py dataset=NCI1 model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
# python train.py dataset=IMDB-BINARY model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
# python train.py dataset=IMDB-MULTI model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
# python train.py dataset=MUTAG model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
# python train.py dataset=REDDIT-BINARY model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun