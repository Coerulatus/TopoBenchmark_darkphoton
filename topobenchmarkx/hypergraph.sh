# EDHNN
python train.py dataset=cocitation_cora model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_citeseer model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_pubmed model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=ZINC model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=PROTEINS_TU model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0 logger.wandb.project=topobenchmark_0503 --multirun

python train.py dataset=NCI1 model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=IMDB-BINARY model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=IMDB-MULTI model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=MUTAG model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=REDDIT-BINARY model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun


# AllDeepSet
# Batch size = 1
python train.py dataset=cocitation_cora model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=50 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_citeseer model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=50 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_pubmed model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=50 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
# Vary batch size
python train.py dataset=PROTEINS_TU model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=NCI1 model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=IMDB-BINARY model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=IMDB-MULTI model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=MUTAG model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=REDDIT-BINARY model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
# Fixed split
python train.py dataset=ZINC model=hypergraph/alldeepset model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.mlp_num_layers=1,2 model.backbone.layer_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun


# UniGNNII
# Batch size = 1
python train.py dataset=cocitation_cora model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=32,64,128 model.backbone.n_layers=1,2 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=50 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_citeseer model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=32,64,128 model.backbone.n_layers=1,2 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=50 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_pubmed model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=32,64,128 model.backbone.n_layers=1,2 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=50 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
# Vary batch size
python train.py dataset=PROTEINS_TU model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=NCI1 model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=IMDB-BINARY model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=IMDB-MULTI model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=MUTAG model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=REDDIT-BINARY model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0,3,5 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun
# Fixed split
python train.py dataset=ZINC model=hypergraph/unignn2 model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=16,32,64,128 model.backbone.n_layers=1,2,3,4 model.backbone.input_drop=0,0.25,0.5 model.backbone.layer_drop=0,0.25,0.5 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0 callbacks.early_stopping.patience=30 trainer=default logger.wandb.project=topobenchmark_0503 --multirun