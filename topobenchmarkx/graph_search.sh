# TO RUN
python train.py dataset=cocitation_cora model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_citeseer model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=cocitation_pubmed model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=50 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=PROTEINS_TU model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 dataset.parameters.batch_size=128,256 logger.wandb.project=topobenchmark_0503 --multirun
python train.py dataset=NCI1 model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 dataset.parameters.batch_size=128,256 logger.wandb.project=topobenchmark_0503 --multirun

# python train.py dataset=IMDB-BINARY model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 dataset.parameters.batch_size=128,256 logger.wandb.project=topobenchmark_0503 --multirun
# python train.py dataset=IMDB-MULTI model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 dataset.parameters.batch_size=128,256 logger.wandb.project=topobenchmark_0503 --multirun
# python train.py dataset=MUTAG model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 dataset.parameters.batch_size=32,64 logger.wandb.project=topobenchmark_0503 --multirun
# python train.py dataset=REDDIT-BINARY model=graph/gin model.optimizer.lr=0.01,0.001 model.backbone.hidden_channels=64,128,256 model.backbone.num_layers=1,2,3,4 dataset.parameters.data_seed=0,3,5 model.backbone.dropout=0,0.25,0.5 callbacks.early_stopping.patience=30 dataset.parameters.data_seed=0,3,5 dataset.parameters.batch_size=128,256 logger.wandb.project=topobenchmark_0503 --multirun
# python train.py dataset=ZINC model=graph/gin model.optimizer.lr=0.01,0.001 model.optimizer.weight_decay=0 model.backbone.hidden_channels=16,32,64,128 model.backbone.num_layers=1,2,3,4 dataset.parameters.batch_size=128,256 dataset.parameters.data_seed=0 model.backbone.dropout=0,0.25,0.5 logger.wandb.project=topobenchmark_0503 callbacks.early_stopping.patience=30 --multirun


