python train.py dataset=cocitation_cora model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=25 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_search --multirun
python train.py dataset=cocitation_citeseer model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=25 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_search --multirun
python train.py dataset=cocitation_pubmed model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=64,128,256 model.backbone.All_num_layers=1,2 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 callbacks.early_stopping.patience=25 dataset.parameters.data_seed=0,3,5 logger.wandb.project=topobenchmark_search --multirun

python train.py dataset=ZINC model=hypergraph/edgnn model.optimizer.lr=0.01,0.001 model.backbone.num_features=16,32,64,128 model.backbone.All_num_layers=1,2,3 model.backbone.MLP_num_layers=0,1,2 model.backbone.input_dropout=0,0.25,0.5 dataset.parameters.batch_size=128,256 callbacks.early_stopping.patience=5 dataset.parameters.data_seed=0 logger.wandb.project=topobenchmark_search --multirun

