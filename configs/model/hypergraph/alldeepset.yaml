_target_: topobenchmarkx.models.network_module.NetworkModule

model_name: alldeepset

feature_encoder:
  _target_: topobenchmarkx.models.encoders.default_encoders.BaseFeatureEncoder
  in_channels: ${infer_in_channels:${dataset}} #${dataset.parameters.num_features}
  out_channels: 32

backbone:
  _target_: topomodelx.nn.hypergraph.allset.AllSet
  in_channels: ${model.feature_encoder.out_channels}
  hidden_channels: ${model.feature_encoder.out_channels}
  n_layers: 1
  layer_dropout: 0.0
  mlp_num_layers: 2
  mlp_dropout: 0.2
  mlp_activation:
    _target_: torch.nn.ReLU
    _partial_: true
  mlp_norm:
    _target_: torch.nn.BatchNorm1d
    _partial_: true
    #num_features: ${model.backbone.hidden_channels}

loss:
  _target_: topobenchmarkx.models.losses.loss.DefaultLoss
  task: ${dataset.parameters.task}
  loss_type: ${dataset.parameters.loss_type}

readout:
  _target_: topobenchmarkx.models.readouts.default_readouts.GNNBatchReadOut
  task_level: ${dataset.parameters.task_level}
  in_channels: ${model.backbone.hidden_channels}
  out_channels: ${dataset.parameters.num_classes}

backbone_wrapper:
  _target_: topobenchmarkx.models.wrappers.default_wrapper.HypergraphWrapper
  _partial_: true

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  _partial_: true
  step_size: 50
  gamma: 0.5

# compile model for faster training with pytorch 2.0
compile: false
