{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 Adding a Custom Dataset Tutorial\n",
    "\n",
    "## 🎯 Tutorial Overview\n",
    "\n",
    "This comprehensive guide walks you through the process of integrating your custom dataset into our library. The process is divided into three main steps:\n",
    "\n",
    "1. **Dataset Creation** 🔨\n",
    "   - Implement data loading mechanisms\n",
    "   - Define preprocessing steps\n",
    "   - Structure data in the required format\n",
    "\n",
    "2. **Integrate with Dataset APIs** 🔄\n",
    "   - Add dataset to the library framework\n",
    "   - Ensure compatibility with existing systems\n",
    "   - Set up proper inheritance structure\n",
    "\n",
    "3. **Configuration Setup** ⚙️\n",
    "   - Define dataset parameters\n",
    "   - Specify data paths and formats\n",
    "   - Configure preprocessing options\n",
    "\n",
    "## 📋 Tutorial Structure\n",
    "\n",
    "This tutorial follows a unique structure to provide the clearest possible learning experience:\n",
    "\n",
    "> 💡 **Main Notebook (Current File)**\n",
    "> - High-level concepts and explanations\n",
    "> - Step-by-step workflow description\n",
    "> - References to implementation files\n",
    "\n",
    "> 📁 **Supporting Files**\n",
    "> - Detailed code implementations\n",
    "> - Specific examples and use cases\n",
    "> - Technical documentation\n",
    "\n",
    "### 🛠️ Technical Framework\n",
    "\n",
    "This tutorial demonstrates custom dataset integration using:\n",
    "- `torch_geometric.data.InMemoryDataset` as the base class\n",
    "- <TBX_name> library's dataset management system\n",
    "\n",
    "### 🎓 Important Notes\n",
    "\n",
    "- To make the learning process concrete, we'll work with a practical toy \"language\" dataset example:\n",
    "- While we use the \"language\" dataset as an example, all file references use the generic `<dataset_name>` format for better generalization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Create a Dataset 🛠️\n",
    "\n",
    "## Overview\n",
    "\n",
    "Adding your custom dataset to <TBX_name> requires implementing specific loading and preprocessing functionality. We utilize the `torch_geometric.data.InMemoryDataset` interface to make this process straightforward.\n",
    "\n",
    "## Required Methods\n",
    "\n",
    "To implement your dataset, you need to override two key methods from the `torch_geometric.data.InMemoryDataset` class:\n",
    "\n",
    "- `download()`: Handles dataset acquisition\n",
    "- `process()`: Manages data preprocessing\n",
    "\n",
    "> 💡 **Reference Implementation**: For a complete example, check `topobenchmarkx/data/datasets/language_dataset.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: The Download Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `download()` method is responsible for acquiring dataset files from external resources. Let's examine its implementation using our language dataset example, where we store data in a GoogleDrive-hosted zip file.\n",
    "\n",
    "#### Implementation Steps\n",
    "\n",
    "1. **Download Data** 📥\n",
    "  - Fetch data from the specified source URL\n",
    "  - Save to the raw directory\n",
    "\n",
    "2. **Extract Content** 📦\n",
    "  - Unzip the downloaded file\n",
    "  - Place contents in appropriate directory\n",
    "\n",
    "3. **Organize Files** 📂\n",
    "  - Move extracted files to named folders\n",
    "  - Clean up temporary files and directories\n",
    "\n",
    "#### Code Implementation\n",
    "\n",
    "```python\n",
    "def download(self) -> None:\n",
    "    # Step 1: download data from source\n",
    "    self.url = self.URLS[self.name]\n",
    "    self.file_format = self.FILE_FORMAT[self.name]\n",
    "    download_file_from_drive(file_link=self.url, path_to_save=self.raw_dir, \n",
    "                           dataset_name=self.name, file_format=self.file_format)\n",
    "\n",
    "    # Step 2: extract zip file\n",
    "    folder = self.raw_dir\n",
    "    filename = f\"{self.name}.{self.file_format}\"\n",
    "    path = osp.join(folder, filename)\n",
    "    extract_zip(path, folder)\n",
    "    os.unlink(path)  # Delete zip\n",
    "    \n",
    "    # Step 3: organize files\n",
    "    for file in os.listdir(osp.join(folder, self.name)):\n",
    "        if file.endswith('ipynb'): continue\n",
    "        shutil.move(osp.join(folder, self.name, file), osp.join(folder, file))\n",
    "    shutil.rmtree(osp.join(folder, self.name))  # Cleanup\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Dive: The Process Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `process()` method handles data preprocessing and organization. Here's the method's structure:\n",
    "\n",
    "```python\n",
    "def process(self) -> None:\n",
    "   r\"\"\"Handle the data for the dataset.\n",
    "   \n",
    "   This method loads the Language dataset, applies preprocessing \n",
    "   transformations, and saves processed data.\"\"\"\n",
    "\n",
    "   # Step 1: extract the data\n",
    "   ...  # Convert raw data to list of torch_geometric.data.Data objects\n",
    "\n",
    "   # Step 2: collate the graphs\n",
    "   self.data, self.slices = self.collate(graph_sentences)\n",
    "\n",
    "   # Step 3: save processed data\n",
    "   fs.torch_save(\n",
    "       (self._data.to_dict(), self.slices, {}, self._data.__class__),\n",
    "       self.processed_paths[0],\n",
    "   )\n",
    "   self.graph = graph_sentences\n",
    "\n",
    "\n",
    "```self.collate``` -- Collates a list of Data or HeteroData objects to the internal storage format; meaning that it transforms a list of torch.data.Data objectis into one torch.data.BaseData.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Integrate with Dataset APIs 🔄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a dataset class, we need to integrate it with the benchmark library. In this section we describe where to add the dataset files and how to make it available through data loaders.\n",
    "\n",
    "\n",
    "Here's how to structure your files, the files highlighted with ** are going to be updated: \n",
    "```\n",
    "topobenchmarkx/\n",
    "├── data/\n",
    "│   ├── datasets/\n",
    "│   │   ├── **init.py**\n",
    "│   │   ├── base.py\n",
    "│   │   ├── <dataset_name>.py   # Your dataset file\n",
    "│   │   └── ...\n",
    "|   ├── loaders\n",
    "│   │   ├── init.py\n",
    "│   │   ├── **loaders.py**\n",
    "```\n",
    "\n",
    "To make your dataset available to library:\n",
    "\n",
    "The file ```<dataset_name>.py```  has been created during the previous steps (`language_dataset.py` in our case) and should be placed in the `topobenchmarkx/data/datasets/` directory. \n",
    "\n",
    "\n",
    "Now it is important Update Registry `topobenchmarkx/data/datasets/__init__.py` to include custom dataset into the library:\n",
    "\n",
    "```python\n",
    "from .<dataset_name> import <dataset_name_class>\n",
    "\n",
    "__all__ = [\n",
    "    # Other datasets...\n",
    "    '<dataset_name_class>',\n",
    "]\n",
    "```\n",
    "\n",
    "Next it is required to update the data loader system. Modify the loader file (`topobenchmarkx/data/loaders/loaders.py`:) to include your dataset:\n",
    "\n",
    "For the the toy example dataset we add the following into the ```load``` method of ```GraphLoader``` class: \n",
    "\n",
    "```python\n",
    "elif self.parameters.data_name in [\"LanguageDataset\"]:\n",
    "   dataset = LanguageDataset(\n",
    "       root=root_data_dir,\n",
    "       name=self.parameters[\"data_name\"],\n",
    "       parameters=self.parameters,\n",
    "   )\n",
    "   \n",
    "   data_dir = dataset.processed_root\n",
    "```\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- In `topobenchmarkx/data/loaders/loaders.py` we additionally provide a template for adding new dataset. \n",
    "- The  ```load``` of class ```GraphLoader``` has to return ```dataset: torch_geometric.data.Dataset``` and ```data_dir: str``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Define Configuration 🔧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've integrated our dataset, we need to define its configuration parameters. In this section, we'll explain how to create and structure the configuration file for your dataset.\n",
    "\n",
    "## Configuration File Structure\n",
    "Create a new YAML file for your dataset in `configs/dataset/<dataset_name>.yaml` with the following structure:\n",
    "\n",
    "\n",
    "### While creating a configuration file, you will need to specify: \n",
    "\n",
    "1) Loader class (`topobenchmarkx.data.loaders.USCountyDemosDatasetLoader`) for automatic instantialization inside the provided pipeline and the parameters for the loader.\n",
    "```yaml\n",
    "# Dataset loader config\n",
    "loader:\n",
    "  _target_: topobenchmarkx.data.loaders.USCountyDemosDatasetLoader\n",
    "  parameters: \n",
    "    data_domain: graph             # Primary data domain. Options: ['graph', 'hypergrpah', 'cell, 'simplicial']\n",
    "    data_type: cornel              # Data type. String emphasizing from where dataset come from. \n",
    "    data_name: US-county-demos     # Name of the dataset\n",
    "    year: 2012                     # In the case of US-county-demos there are multiple version of this dataset. Options:[2012, 2016]\n",
    "    task_variable: 'Election'      # Different target variable used as target. Options: ['Election', 'MedianIncome', 'MigraRate', 'BirthRate', 'DeathRate', 'BachelorRate', 'UnemploymentRate']\n",
    "    data_dir: ${paths.data_dir}/${dataset.loader.parameters.data_domain}/${dataset.loader.parameters.data_type}\n",
    "``` \n",
    "\n",
    "2) The dataset parameters: \n",
    "\n",
    "```yaml\n",
    "# Dataset parameters\n",
    "parameters:\n",
    "  num_features: 6         # Number of features in the dataset\n",
    "  num_classes: 1          # Dimentuin of the target variable\n",
    "  task: regression        # Dataset task. Options: [classification, regression]\n",
    "  loss_type: mse          # Task-specific loss function\n",
    "  monitor_metric: mae     # Metric to monitor during training\n",
    "  task_level: node        # Task level. Options: [classification, regression]\n",
    "```\n",
    "\n",
    "3) The dataset split parameters: \n",
    "```yaml\n",
    "#splits\n",
    "split_params:\n",
    "  learning_setting: transductive      # Type of learning. Options:['transductive', 'inductive']\n",
    "  data_seed: 0                        # Seed for data splitting\n",
    "  split_type: random                  # Type of splitting. Options: ['k-fold', 'random']\n",
    "  k: 10                               # Number of folds in case of \"k-fold\" cross-validation\n",
    "  train_prop: 0.5                     # Training proportion in case of 'random' splitting strategy\n",
    "  standardize: True                   # Standardize the data or not. Options: [True, False]\n",
    "  data_split_dir: ${paths.data_dir}/data_splits/${dataset.loader.parameters.data_name}\n",
    "```\n",
    "\n",
    "4) Finally the dataloader parameters:\n",
    "\n",
    "```yaml\n",
    "# Dataloader parameters\n",
    "dataloader_params:\n",
    "  batch_size: 1       # Number of graphs per batch. In sace of transductive always 1 as there is only one graph. \n",
    "  num_workers: 0      # Number of workers for data loading\n",
    "  pin_memory: False   # Pin memory for data loading\n",
    "```\n",
    "\n",
    "### Notes:\n",
    "- The `paths` section in the configuration file is automatically populated with the paths to the data directory and the data splits directory.\n",
    "- Some of the dataset parameters are used to configure the model.yaml and other files. Hence we suggest always include the above parameters in the dataset configuration file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the markdown for easy copying:\n",
    "\n",
    "\n",
    "## Preparing to Load the Custom Dataset: Understanding Configuration Imports\n",
    "\n",
    "Before loading our dataset, it's crucial to understand the configuration imports, particularly those from the `topobenchmarkx.utils.config_resolvers` module. These utility functions play a key role in dynamically configuring your machine learning pipeline.\n",
    "\n",
    "### Key Imports for Dynamic Configuration\n",
    "\n",
    "Let's import the essential configuration resolver functions:\n",
    "\n",
    "```python\n",
    "from topobenchmarkx.utils.config_resolvers import (\n",
    "    get_default_transform,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    infer_in_channels,\n",
    ")\n",
    "```\n",
    "\n",
    "### Why These Imports Matter\n",
    "\n",
    "In our previous step, we explored configuration variables that use dynamic lookups, such as:\n",
    "\n",
    "```yaml\n",
    "data_dir: ${paths.data_dir}/${dataset.loader.parameters.data_domain}/${dataset.loader.parameters.data_type}\n",
    "```\n",
    "\n",
    "However, some configurations require more advanced automation, which is where these imported functions become invaluable.\n",
    "\n",
    "### Practical Example: Dynamic Transforms\n",
    "\n",
    "Consider the configuration in `projects/TopoBenchmark/configs/run.yaml`, where the `transforms` parameter uses the `get_default_transform` function:\n",
    "\n",
    "```yaml\n",
    "transforms: ${get_default_transform:${dataset},${model}}\n",
    "```\n",
    "\n",
    "This syntax allows for automatic transformation selection based on the dataset and model, demonstrating the power of these configuration resolver functions.\n",
    "\n",
    "By importing and utilizing these functions, you gain:\n",
    "- Flexible configuration management\n",
    "- Automatic parameter inference\n",
    "- Reduced manual configuration overhead\n",
    "\n",
    "These facilitate seamless dataset loading and preprocessing for multiple topological domains and provide an easy and intuitive interface for incorporating novel functionality.\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1148679/3742482502.py:15: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../configs\", job_name=\"job\")\n"
     ]
    },
    {
     "ename": "ConfigCompositionException",
     "evalue": "Error resolving interpolation '${get_default_transform:${dataset},${model}}', possible interpolation keys: debug, hparams_search, experiment, hydra, extras, paths, trainer, logger, callbacks, evaluator, loss, optimizer, model, dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedInterpolationType\u001b[0m              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/core/default_element.py:238\u001b[0m, in \u001b[0;36mInputDefault._resolve_interpolation_impl\u001b[0;34m(self, known_choices, val)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_dummy_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, \u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/dictconfig.py:375\u001b[0m, in \u001b[0;36mDictConfig.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_and_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/base.py:231\u001b[0m, in \u001b[0;36mNode._format_and_raise\u001b[0;34m(self, key, value, cause, msg, type_override)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_and_raise\u001b[39m(\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    225\u001b[0m     key: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     type_override: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[43mformat_and_raise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcause\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtype_override\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtype_override\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/_utils.py:899\u001b[0m, in \u001b[0;36mformat_and_raise\u001b[0;34m(node, key, value, msg, cause, type_override)\u001b[0m\n\u001b[1;32m    897\u001b[0m     ex\u001b[38;5;241m.\u001b[39mref_type_str \u001b[38;5;241m=\u001b[39m ref_type_str\n\u001b[0;32m--> 899\u001b[0m \u001b[43m_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/_utils.py:797\u001b[0m, in \u001b[0;36m_raise\u001b[0;34m(ex, cause)\u001b[0m\n\u001b[1;32m    796\u001b[0m     ex\u001b[38;5;241m.\u001b[39m__cause__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 797\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/dictconfig.py:369\u001b[0m, in \u001b[0;36mDictConfig.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_DEFAULT_MARKER_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/dictconfig.py:451\u001b[0m, in \u001b[0;36mDictConfig._get_impl\u001b[0;34m(self, key, default_value, validate_key)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Node)\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_with_default\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_value\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/basecontainer.py:98\u001b[0m, in \u001b[0;36mBaseContainer._resolve_with_default\u001b[0;34m(self, key, value, default_value)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MissingMandatoryValue(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing mandatory value: $FULL_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m resolved_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_resolve_interpolation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_value(resolved_node)\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/base.py:719\u001b[0m, in \u001b[0;36mContainer._maybe_resolve_interpolation\u001b[0;34m(self, parent, key, value, throw_on_resolution_failure, memo)\u001b[0m\n\u001b[1;32m    718\u001b[0m parse_tree \u001b[38;5;241m=\u001b[39m parse(_get_value(value))\n\u001b[0;32m--> 719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_interpolation_from_parse_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthrow_on_resolution_failure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/base.py:584\u001b[0m, in \u001b[0;36mContainer._resolve_interpolation_from_parse_tree\u001b[0;34m(self, parent, value, key, parse_tree, throw_on_resolution_failure, memo)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     resolved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve_parse_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_tree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemo\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InterpolationResolutionError:\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/base.py:764\u001b[0m, in \u001b[0;36mContainer.resolve_parse_tree\u001b[0;34m(self, parse_tree, node, memo, key)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparse_tree\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InterpolationResolutionError:\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/antlr4/tree/Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit\u001b[39m(\u001b[38;5;28mself\u001b[39m, tree):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/grammar/gen/OmegaConfGrammarParser.py:206\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.ConfigValueContext.accept\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m( visitor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisitConfigValue\u001b[39m\u001b[38;5;124m\"\u001b[39m ):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisitConfigValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/grammar_visitor.py:101\u001b[0m, in \u001b[0;36mGrammarVisitor.visitConfigValue\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mgetChildCount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetChild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/antlr4/tree/Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit\u001b[39m(\u001b[38;5;28mself\u001b[39m, tree):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/grammar/gen/OmegaConfGrammarParser.py:342\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.TextContext.accept\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m( visitor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisitText\u001b[39m\u001b[38;5;124m\"\u001b[39m ):\n\u001b[0;32m--> 342\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisitText\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/grammar_visitor.py:298\u001b[0m, in \u001b[0;36mGrammarVisitor.visitText\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, OmegaConfGrammarParser\u001b[38;5;241m.\u001b[39mInterpolationContext):\n\u001b[0;32m--> 298\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisitInterpolation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Otherwise, concatenate string representations together.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/grammar_visitor.py:125\u001b[0m, in \u001b[0;36mGrammarVisitor.visitInterpolation\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mgetChildCount() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# interpolationNode | interpolationResolver\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetChild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/antlr4/tree/Tree.py:34\u001b[0m, in \u001b[0;36mParseTreeVisitor.visit\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvisit\u001b[39m(\u001b[38;5;28mself\u001b[39m, tree):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/grammar/gen/OmegaConfGrammarParser.py:1041\u001b[0m, in \u001b[0;36mOmegaConfGrammarParser.InterpolationResolverContext.accept\u001b[0;34m(self, visitor)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m( visitor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvisitInterpolationResolver\u001b[39m\u001b[38;5;124m\"\u001b[39m ):\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvisitor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisitInterpolationResolver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/grammar_visitor.py:179\u001b[0m, in \u001b[0;36mGrammarVisitor.visitInterpolationResolver\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         args_str\u001b[38;5;241m.\u001b[39mappend(txt)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolver_interpolation_callback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolver_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/base.py:750\u001b[0m, in \u001b[0;36mContainer.resolve_parse_tree.<locals>.resolver_interpolation_callback\u001b[0;34m(name, args, args_str)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresolver_interpolation_callback\u001b[39m(\n\u001b[1;32m    748\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m, args: Tuple[Any, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], args_str: Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[1;32m    749\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_custom_resolver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    752\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43minter_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43minter_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43minter_args_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/omegaconf/base.py:702\u001b[0m, in \u001b[0;36mContainer._evaluate_custom_resolver\u001b[0;34m(self, key, node, inter_type, inter_args, inter_args_str)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnsupportedInterpolationType(\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported interpolation type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minter_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m     )\n",
      "\u001b[0;31mUnsupportedInterpolationType\u001b[0m: Unsupported interpolation type get_default_transform\n    full_key: _dummy_\n    object_type=dict",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConfigCompositionException\u001b[0m                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhydra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m instantiate\n\u001b[1;32m     15\u001b[0m initialize(config_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../configs\u001b[39m\u001b[38;5;124m\"\u001b[39m, job_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mcompose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel=hypergraph/unignn2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset=hypergraph/coauthorship_cora\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_hydra_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loader \u001b[38;5;241m=\u001b[39m instantiate(cfg\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mloader)\n\u001b[1;32m     27\u001b[0m dataset, dataset_dir \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/compose.py:38\u001b[0m, in \u001b[0;36mcompose\u001b[0;34m(config_name, overrides, return_hydra_config, strict)\u001b[0m\n\u001b[1;32m     36\u001b[0m gh \u001b[38;5;241m=\u001b[39m GlobalHydra\u001b[38;5;241m.\u001b[39minstance()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m gh\u001b[38;5;241m.\u001b[39mhydra \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mgh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhydra\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompose_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRunMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRUN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_shell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_log_configuration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cfg, DictConfig)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_hydra_config:\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/hydra.py:594\u001b[0m, in \u001b[0;36mHydra.compose_config\u001b[0;34m(self, config_name, overrides, run_mode, with_log_configuration, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompose_config\u001b[39m(\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    578\u001b[0m     config_name: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    583\u001b[0m     validate_sweep_overrides: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    584\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DictConfig:\n\u001b[1;32m    585\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;124;03m    :param config_name:\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;124;03m    :param overrides:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 594\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_configuration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_shell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_shell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_sweep_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_sweep_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m with_log_configuration:\n\u001b[1;32m    602\u001b[0m         configure_log(cfg\u001b[38;5;241m.\u001b[39mhydra\u001b[38;5;241m.\u001b[39mhydra_logging, cfg\u001b[38;5;241m.\u001b[39mhydra\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/config_loader_impl.py:142\u001b[0m, in \u001b[0;36mConfigLoaderImpl.load_configuration\u001b[0;34m(self, config_name, overrides, run_mode, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_configuration\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     config_name: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     validate_sweep_overrides: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    140\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DictConfig:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_configuration_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfrom_shell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_shell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_sweep_overrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_sweep_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m OmegaConfBaseException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigCompositionException()\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/config_loader_impl.py:253\u001b[0m, in \u001b[0;36mConfigLoaderImpl._load_configuration_impl\u001b[0;34m(self, config_name, overrides, run_mode, from_shell, validate_sweep_overrides)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_sweep_overrides:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_sweep_overrides_legal(\n\u001b[1;32m    250\u001b[0m         overrides\u001b[38;5;241m=\u001b[39mparsed_overrides, run_mode\u001b[38;5;241m=\u001b[39mrun_mode, from_shell\u001b[38;5;241m=\u001b[39mfrom_shell\n\u001b[1;32m    251\u001b[0m     )\n\u001b[0;32m--> 253\u001b[0m defaults_list \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_defaults_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaching_repo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverrides_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparsed_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepend_hydra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mRunMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMULTIRUN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m config_overrides \u001b[38;5;241m=\u001b[39m defaults_list\u001b[38;5;241m.\u001b[39mconfig_overrides\n\u001b[1;32m    263\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compose_config_from_defaults_list(\n\u001b[1;32m    264\u001b[0m     defaults\u001b[38;5;241m=\u001b[39mdefaults_list\u001b[38;5;241m.\u001b[39mdefaults, repo\u001b[38;5;241m=\u001b[39mcaching_repo\n\u001b[1;32m    265\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:745\u001b[0m, in \u001b[0;36mcreate_defaults_list\u001b[0;34m(repo, config_name, overrides_list, prepend_hydra, skip_missing)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m:param repo:\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03m:param config_name:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    744\u001b[0m overrides \u001b[38;5;241m=\u001b[39m Overrides(repo\u001b[38;5;241m=\u001b[39mrepo, overrides_list\u001b[38;5;241m=\u001b[39moverrides_list)\n\u001b[0;32m--> 745\u001b[0m defaults, tree \u001b[38;5;241m=\u001b[39m \u001b[43m_create_defaults_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepend_hydra\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_hydra\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m overrides\u001b[38;5;241m.\u001b[39mensure_overrides_used()\n\u001b[1;32m    753\u001b[0m overrides\u001b[38;5;241m.\u001b[39mensure_deletions_used()\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:715\u001b[0m, in \u001b[0;36m_create_defaults_list\u001b[0;34m(repo, config_name, overrides, prepend_hydra, skip_missing)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_defaults_list\u001b[39m(\n\u001b[1;32m    707\u001b[0m     repo: IConfigRepository,\n\u001b[1;32m    708\u001b[0m     config_name: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    711\u001b[0m     skip_missing: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    712\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[ResultDefault], DefaultsTreeNode]:\n\u001b[1;32m    713\u001b[0m     root \u001b[38;5;241m=\u001b[39m _create_root(config_name\u001b[38;5;241m=\u001b[39mconfig_name, with_hydra\u001b[38;5;241m=\u001b[39mprepend_hydra)\n\u001b[0;32m--> 715\u001b[0m     defaults_tree \u001b[38;5;241m=\u001b[39m \u001b[43m_create_defaults_tree\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_root_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolated_subtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m     output \u001b[38;5;241m=\u001b[39m _tree_to_list(tree\u001b[38;5;241m=\u001b[39mdefaults_tree)\n\u001b[1;32m    725\u001b[0m     ensure_no_duplicates_in_list(output)\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:356\u001b[0m, in \u001b[0;36m_create_defaults_tree\u001b[0;34m(repo, root, is_root_config, skip_missing, interpolated_subtree, overrides)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_defaults_tree\u001b[39m(\n\u001b[1;32m    349\u001b[0m     repo: IConfigRepository,\n\u001b[1;32m    350\u001b[0m     root: DefaultsTreeNode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m     overrides: Overrides,\n\u001b[1;32m    355\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DefaultsTreeNode:\n\u001b[0;32m--> 356\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_create_defaults_tree_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_root_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_root_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolated_subtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolated_subtree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:457\u001b[0m, in \u001b[0;36m_create_defaults_tree_impl\u001b[0;34m(repo, root, is_root_config, skip_missing, interpolated_subtree, overrides)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_virtual():\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_root_config:\n\u001b[0;32m--> 457\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_expand_virtual_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_missing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:280\u001b[0m, in \u001b[0;36m_expand_virtual_root\u001b[0;34m(repo, root, overrides, skip_missing)\u001b[0m\n\u001b[1;32m    277\u001b[0m new_root \u001b[38;5;241m=\u001b[39m DefaultsTreeNode(node\u001b[38;5;241m=\u001b[39md, parent\u001b[38;5;241m=\u001b[39mroot)\n\u001b[1;32m    278\u001b[0m d\u001b[38;5;241m.\u001b[39mupdate_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m subtree \u001b[38;5;241m=\u001b[39m \u001b[43m_create_defaults_tree_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_root_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolated_subtree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subtree\u001b[38;5;241m.\u001b[39mchildren \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     children\u001b[38;5;241m.\u001b[39mappend(d)\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/_internal/defaults_list.py:580\u001b[0m, in \u001b[0;36m_create_defaults_tree_impl\u001b[0;34m(repo, root, is_root_config, skip_missing, interpolated_subtree, overrides)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, dd \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(children):\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dd, InputDefault) \u001b[38;5;129;01mand\u001b[39;00m dd\u001b[38;5;241m.\u001b[39mis_interpolation():\n\u001b[0;32m--> 580\u001b[0m         \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolve_interpolation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_choices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m         new_root \u001b[38;5;241m=\u001b[39m DefaultsTreeNode(node\u001b[38;5;241m=\u001b[39mdd, parent\u001b[38;5;241m=\u001b[39mroot)\n\u001b[1;32m    582\u001b[0m         dd\u001b[38;5;241m.\u001b[39mupdate_parent(parent\u001b[38;5;241m.\u001b[39mget_group_path(), parent\u001b[38;5;241m.\u001b[39mget_final_package())\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/core/default_element.py:558\u001b[0m, in \u001b[0;36mGroupDefault.resolve_interpolation\u001b[0;34m(self, known_choices)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConfigCompositionException(msg)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_interpolation_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_choices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tb/lib/python3.11/site-packages/hydra/core/default_element.py:252\u001b[0m, in \u001b[0;36mInputDefault._resolve_interpolation_impl\u001b[0;34m(self, known_choices, val)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError resolving interpolation \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ConfigCompositionException(msg)\n",
      "\u001b[0;31mConfigCompositionException\u001b[0m: Error resolving interpolation '${get_default_transform:${dataset},${model}}', possible interpolation keys: debug, hparams_search, experiment, hydra, extras, paths, trainer, logger, callbacks, evaluator, loss, optimizer, model, dataset"
     ]
    }
   ],
   "source": [
    "from hydra import compose, initialize\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "\n",
    "\n",
    "from topobenchmarkx.utils.config_resolvers import (\n",
    "    get_default_transform,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    infer_in_channels,\n",
    ")\n",
    "\n",
    "\n",
    "initialize(config_path=\"../configs\", job_name=\"job\")\n",
    "cfg = compose(\n",
    "    config_name=\"run.yaml\",\n",
    "    overrides=[\n",
    "        \"model=hypergraph/unignn2\",\n",
    "        \"dataset=hypergraph/coauthorship_cora\",\n",
    "    ], \n",
    "    return_hydra_config=True\n",
    ")\n",
    "loader = instantiate(cfg.dataset.loader)\n",
    "\n",
    "\n",
    "dataset, dataset_dir = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Custom Data Transformations ⚙️\n",
    "\n",
    "While most datasets can be used directly after integration, some require specific preprocessing transformations. These transformations might vary depending on the task, model, or other conditions.\n",
    "\n",
    "## Example Case: Language Dataset\n",
    "\n",
    "Let's look at our language dataset's structure:\n",
    "- Each graph represents an English sentence\n",
    "- Nodes are tokens i.e. each node is a string and it doesn't have corresponding high dimentional feature embedding\n",
    "- Edges come from transformer attention, hence forming a fully connected graph\n",
    "\n",
    "For this dataset, two default transformations are logical:\n",
    "1. **Graph Sparsification**: Reduce edge density\n",
    "2. **Node Feature Generation**: Create numerical features from tokens\n",
    "\n",
    "\n",
    "Below we provide an quick tutorial on how to create a data transformations and create a sequence of default transformations that will be executed whener you use the defined dataset config file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Transform\n",
    "\n",
    "In general any transfom in the library inherits `torch_geometric.transforms.BaseTransform` class, which allow to apply a sequency of transforms to the data. Our inderface requires to implement the `forward` method. The important part of all transforms is that it takes `torch_geometric.data.Data` object and returns updated `torch_geometric.data.Data` object.\n",
    "\n",
    "\n",
    "\n",
    "For language dataset,  we have generated the `attention2graph` transfroms that is a data_manipulation transform hence we place it into `topobenchmarkx/transforms/data_manipulation/` folder. \n",
    "Below you can see the `forward` method of `Attention2Graph` class: \n",
    "\n",
    "\n",
    "```python\n",
    "   def forward(self, data: torch_geometric.data.Data):\n",
    "       # Reshape attention scores\n",
    "       attention_shape = data.attention_shape\n",
    "       attention_scores = data.attention_scores.reshape(attention_shape)\n",
    "       \n",
    "       # Apply threshold\n",
    "       mask = attention_scores > self.parameters[\"threshold\"]\n",
    "       edge_index = torch.stack(torch.where(mask==1))\n",
    "       \n",
    "       # Process edges\n",
    "       edge_index = torch_geometric.utils.remove_self_loops(edge_index)[0]\n",
    "       edge_index = torch_geometric.utils.to_undirected(edge_index)\n",
    "       data.edge_index = edge_index\n",
    "       \n",
    "       return data\n",
    "```\n",
    "\n",
    "Please see the `topobenchmarkx/transforms/data_manipulation/attention2graph.py` file for the precise implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Transform\n",
    "\n",
    "Similarly as adding dataset we have to registed the transform we have created, to do so please follow the procedure below:\n",
    "\n",
    "Update `topobenchmarkx/transforms/data_manipulations/__init__.py`:\n",
    "\n",
    "``` python\n",
    "# Step 1: Import your transform\n",
    "from .attention2graph import Attention2Graph\n",
    "\n",
    "# Step 2: Add to DATA_MANIPULATIONS dictionary\n",
    "DATA_MANIPULATIONS = {\n",
    "    \"Identity\": IdentityTransform,\n",
    "    # ... other transforms ...\n",
    "    \"Attention2Graph\": Attention2Graph,  # Add your transform\n",
    "}\n",
    "\n",
    "# Step 3: Add to __all__\n",
    "__all__ = [\n",
    "    \"DATA_MANIPULATIONS\",\n",
    "    # ... other transforms ...\n",
    "    \"Attention2Graph\"  # Add your transform\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a configuration file \n",
    "Now as we have registered the transform we can finally create the configuration file and use it in the framework: \n",
    "\n",
    "``` yaml\n",
    "_target_: topobenchmarkx.transforms.data_transform.DataTransform\n",
    "transform_name: \"Attention2Graph\"\n",
    "transform_type: \"data manipulation\"\n",
    "threshold: 0.1\n",
    "``` \n",
    "Please refer to `configs/transforms/dataset_defaults/attention2graph.yaml` for the example. \n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- You might notice an interesting key `_target_` in the configuration file. In general for any new transform you the `_target_` is always `topobenchmarkx.transforms.data_transform.DataTransform`.  [For more information please refer to hydra documentation \"Instantiating objects with Hydra\" section.](https://hydra.cc/docs/advanced/instantiate_objects/overview/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default transforms\n",
    "\n",
    "Now when we have crated the transfor we can define a list of default transforms that will be executed always whenwever the dataset under default configuration is used.\n",
    "\n",
    "\n",
    "To configure the deafult transform navigate to `configs/transforms/dataset_defaults` create `<def_transforms.yaml>` and the follwoing `.yaml` file: \n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - transform_1: transform_1\n",
    "  - transform_2: transform_2\n",
    "  - transform_3: transform_3\n",
    "```\n",
    "\n",
    "\n",
    "**Important**\n",
    "There are different types of transforms, including `data_manipulation`, `liftings`, and `feature_liftings`. In case you want to use multiple transforms from the same categoty, let's say from `data_manipulation`, then it is required to stick to a special syntaxis. [See hydra configuration for more information]() or the example below: \n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - data_manipulation@first_usage: transform_1\n",
    "  - data_manipulation@second_usage: transform_2\n",
    "```\n",
    "\n",
    "\n",
    "In the case of the language dataset we have the following `language.yaml` file:\n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "  - data_manipulations@equal_gaus_features: equal_gaus_features\n",
    "  - data_manipulations@attention2graph: attention2graph\n",
    "  - liftings@_here_: ${get_required_lifting:graph,${model}}\n",
    "\n",
    "equal_gaus_features:\n",
    "  num_features: 10\n",
    "```\n",
    "\n",
    "\n",
    "In our example we have a bunch of interesting aspects: \n",
    "- There are a two transforms from the same catgory `data_manipulations`, hence we use operator `@` to assign new names `equal_gaus_features` and `attention2graph`\n",
    "-  In the case of `equal_gaus_features` we have to override the initial number of features as the `equal_gaus_features.yaml` uses a special \"configuration register\" to infer the feature dimension. In the case of our language dataset, we don't have node's hidden features, hence we need to define the number of features of our own. \n",
    "- We recommend to add `liftings@_here_: ${get_required_lifting:graph,${model}}` so that a default lifting is applied to run any domain-specific topological model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Add language_dataset.py file in topobenchmarkx/data/datasets folder\n",
    "\n",
    "# Step 2\n",
    "# Update __init__.py file in topobenchmarkx/data/datasets\n",
    "# Update loaders.py in topobenchmarkx/data/loaders\n",
    "\n",
    "# Step 3\n",
    "# Create confin file TopoBenchmark/configs/dataset/graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Preprocessing: \n",
    "# Optional: in case you need to have some default transform, create them following 1. and generate corresponding .yaml configuration files and add to configs/transforms/dataset_defaults folder \n",
    "\n",
    "\n",
    "# To create a transform see steps below\n",
    "# Step 1: Crate transform requitred for the dataset add it to appropriate topobenchmarkx/transforms folder\n",
    "# Step 1.1:\n",
    "# Step 1.2:\n",
    "# Step 1.3: \n",
    "# Step 1.4: Add configuration file associated with the transform into transforms/data_manipulations (see file attention2graph.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1117304/3249548314.py:24: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  initialize(config_path=\"../configs\", job_name=\"job\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting /home/lev/projects/TopoBenchmark/datasets/hypergraph/coauthorship/coauthorship_cora/raw/coauthorship_cora.zip\n"
     ]
    }
   ],
   "source": [
    "import rootutils\n",
    "\n",
    "rootutils.setup_root(\"./\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "import torch_geometric\n",
    "from hydra import compose, initialize\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from topobenchmarkx.data.preprocessor import PreProcessor\n",
    "from topobenchmarkx.dataloader.dataloader import TBXDataloader\n",
    "\n",
    "from topobenchmarkx.utils.config_resolvers import (\n",
    "    get_default_transform,\n",
    "    get_monitor_metric,\n",
    "    get_monitor_mode,\n",
    "    infer_in_channels,\n",
    ")\n",
    "\n",
    "initialize(config_path=\"../configs\", job_name=\"job\")\n",
    "cfg = hydra.compose(\n",
    "    config_name=\"run.yaml\",\n",
    "    overrides=[\n",
    "        \"model=hypergraph/unignn2\",\n",
    "        \"dataset=hypergraph/coauthorship_cora\",\n",
    "    ], \n",
    "    return_hydra_config=True\n",
    ")\n",
    "loader = hydra.utils.instantiate(cfg.dataset.loader)\n",
    "\n",
    "\n",
    "dataset, dataset_dir = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 4905], y=[2708], x_0=[2708, 1433], incidence_hyperedges=[2708, 1392], n_x=2708, num_hyperedges=1392, num_class=7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
